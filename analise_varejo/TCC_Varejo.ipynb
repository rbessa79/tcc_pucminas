{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Conteúdo:\n",
    "* [Analisando os Dados](#1)\n",
    "* [Analisando os dados com base na linha do tempo](#2)\n",
    "* [Análise nível Loja](#3)\n",
    "* [Analise nível Departamento](#4)\n",
    "* [Análise de Loja por tipo e por Ano/Mês](#5) \n",
    "* [Análise MarkDown](#6)\n",
    "* [Previsão por Data e Loja](#7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import requests\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Arquivo com informações de loja\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_store \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/rbessa79/tcc_pucminas/main/data/stores-data-set.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Arquivo com informações complementares\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_feature \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/rbessa79/tcc_pucminas/main/data/Features-data-set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:670\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    667\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 670\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    679\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:339\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    338\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    340\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:239\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py:1350\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1350\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1352\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto"
     ]
    }
   ],
   "source": [
    "# Arquivo com informações de loja\n",
    "df_store = pd.read_csv('https://raw.githubusercontent.com/rbessa79/tcc_pucminas/main/data/stores-data-set.csv')\n",
    "\n",
    "# Arquivo com informações complementares\n",
    "df_feature = pd.read_csv(\"https://raw.githubusercontent.com/rbessa79/tcc_pucminas/main/data/Features-data-set.csv\", parse_dates=['Date'])\n",
    "\n",
    "# Arquivo com informações de vendas\n",
    "df_sales = pd.read_csv(\"https://raw.githubusercontent.com/rbessa79/tcc_pucminas/main/data/sales-data-set.csv\", parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# df_store = pd.read_csv(\"C:\\\\Users\\\\rbessa\\\\Downloads\\\\archive\\\\stores data-set.csv\")\n",
    "# df_feature = pd.read_csv(\"C:\\\\Users\\\\rbessa\\\\Downloads\\\\archive\\\\Features data set.csv\",parse_dates=[\"Date\"])\n",
    "# df_sales = pd.read_csv(\"C:\\\\Users\\\\rbessa\\\\Downloads\\\\archive\\\\sales data-set.csv\",parse_dates=[\"Date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "# Analisando os dados\n",
    "\n",
    "verificando as tabelas e informações básicas\n",
    "* Tipo dos dados\n",
    "* Número de dados (linhas e colunas)\n",
    "* Dados nulos\n",
    "* Algumas linhas para entender os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_store.info())\n",
    "print(df_store.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature.info())\n",
    "print(df_feature.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sales.info())\n",
    "print(df_sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise prelimiar dos dados, chego à conclusão de que podemos agrupar dados com base no tempo, loja, departamento.\n",
    "Também podemos verificar como feriados, temperatura, preço do combustível, CPI, desemprego e tipo de loja afetam as vendas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "# **Analisando os dados com base na linha do tempo**\n",
    "\n",
    "* Jutando as tabelas Features e Sales baseados na data \n",
    "* Converter vendas semanais em milhões\n",
    "* Modificando a coluna conforme o dia é feriado ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_date = df_feature.groupby(\"Date\").agg({\"Temperature\":\"mean\"\n",
    "                                            ,\"Fuel_Price\":\"mean\"\n",
    "                                            ,\"IsHoliday\":\"sum\"\n",
    "                                            ,\"CPI\":\"mean\"\n",
    "                                           ,\"Unemployment\":\"mean\"})\n",
    "data_date = data_date.sort_index()\n",
    "temp_date_data = data_date[:'2012-12-10']\n",
    "\n",
    "data_sales_date = df_sales.groupby(\"Date\").agg({\"Weekly_Sales\":\"sum\"})\n",
    "data_sales_date.sort_index(inplace=True)\n",
    "data_sales_date.Weekly_Sales = data_sales_date.Weekly_Sales/1000000\n",
    "data_sales_date.Weekly_Sales = data_sales_date.Weekly_Sales.apply(int)\n",
    "data = pd.merge(data_sales_date, temp_date_data, left_index=True,right_index=True, how='left')\n",
    "data[\"IsHoliday\"] = data[\"IsHoliday\"].apply(lambda x: True if x == 45.0 else False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(5,1,figsize=(15,10),sharex=True) \n",
    "data[\"Weekly_Sales\"].plot(ax=ax[0],title=\"Weekly Sales/sales on Holiday\")\n",
    "data[data.IsHoliday==True][\"Weekly_Sales\"].plot(marker=\"D\",ax=ax[0],legend=\"Holiday Week sale\")\n",
    "data[\"Temperature\"].plot(ax=ax[1], title=\"Temperature\")\n",
    "data[\"Fuel_Price\"].plot(ax=ax[2],title=\"Fuel_Price\")\n",
    "data[\"CPI\"].plot(ax=ax[3],title=\"CPI\")\n",
    "data[\"Unemployment\"].plot(ax=ax[4],title=\"Unemployment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nov- Dec mostram aumento nas vendas semanais, mas ao longo do ano não há crescimento. \n",
    "* Semanas que antecedem feriados mostram picos de vendas\n",
    "* O Preço dos Combustíveis e o Índice de Preços ao Consumidor apresentaram crescimento ao longo do ano.\n",
    "* O desemprego diminuiu ano após ano.\n",
    "* A temperatura está mostrando uma caminhada aleatória -- VERIFICAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As vendas semanais não mostram nenhuma correlação alta com nenhum outro parâmetro.\n",
    "* CPI e Desemprego mostram correlação negativa, por outro lado, o preço do combustível mostra correlação positiva.\n",
    "* O desemprego e o preço do combustível também estão negativamente correlacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales_month = data.groupby(data.index.month).agg({\"Weekly_Sales\":\"sum\"})\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=data_sales_month.index,y=data_sales_month.Weekly_Sales)\n",
    "plt.title(\"Month wise Sales\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pode ser o suficiente para comprar durante as ofertas e feriados (vendas na Black Friday, Natal, Ano Novo) e as vendas posteriores caem e, em seguida, ganham impulso novamente em alguns meses.\n",
    "* Em novembro, também as pessoas mostram que menos compras podem ser um período de espera para as próximas vendas. --VERIFICAR\n",
    "* Os dados completos para dezembro de 2013, acho que não estão disponíveis, portanto, mesmo após o pico em dezembro, as vendas não subiram muito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales_year = data.groupby(data.index.year).agg({\"Weekly_Sales\":\"sum\"})\n",
    "\n",
    "sns.barplot(x=data_sales_year.index,y=data_sales_year.Weekly_Sales)\n",
    "plt.title(\"Year wise Sales\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(data[\"Weekly_Sales\"], period=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(decomposition.trend)\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.plot(decomposition.resid)\n",
    "plt.legend([\"Trend\", \"Seasonal\",\"Resid\"], loc =\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui mostra uma tendência lateral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "# **Análise nível Loja**\n",
    "\n",
    "vamos explorar o padrão armazenado para dados de vendas semanais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Store = df_feature.groupby(\"Store\").agg({\"Temperature\":\"mean\",\"Fuel_Price\":\"mean\",\"IsHoliday\":\"sum\"})\n",
    "\n",
    "temp_store = df_sales.groupby(\"Store\").agg({\"Weekly_Sales\":\"sum\"})\n",
    "temp_store.Weekly_Sales = temp_store.Weekly_Sales/1000000\n",
    "temp_store.Weekly_Sales = temp_store.Weekly_Sales.apply(int)\n",
    "data_Store.set_index(np.arange(0,45),inplace=True)\n",
    "df_store[\"temp\"] = data_Store.Temperature\n",
    "df_store[\"Fuel_Price\"] = data_Store.Fuel_Price\n",
    "df_store[\"holiday\"] = data_Store.IsHoliday\n",
    "df_store[\"Weekly_Sales\"] = temp_store.Weekly_Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(15, 4))\n",
    "sns.countplot(df_store.Type,ax=ax[0])\n",
    "sns.swarmplot(data = df_store,y=\"Size\",x=\"Type\",ax=ax[1])\n",
    "\n",
    "sns.boxplot(data = df_store,y=\"Weekly_Sales\",x=\"Type\",ax=ax[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contagem de lojas está na ordem de A, B e menos é C\n",
    "* O tamanho de Lojas tipo A armazena a maioria dos itens seguidos por B e depois por C --VERIFICAR\n",
    "* Mesmo depois de menos contagem de lojas e menor tamanho B, tem mais vendas semanais do que A e C, pelo menos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "# **Analise nível Departamento**\n",
    "\n",
    "Uma pequena investigação por departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sales[\"Dept\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "data_Dept = df_sales.groupby(\"Dept\").agg({\"Weekly_Sales\":\"sum\"})\n",
    "data_Dept.Weekly_Sales = data_Dept.Weekly_Sales/10000\n",
    "data_Dept.Weekly_Sales = data_Dept.Weekly_Sales.apply(int)\n",
    "data_Dept.sort_values(by=\"Weekly_Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(15, 4))\n",
    "plt.vlines(x=data_Dept.index, ymin=0, ymax=data_Dept['Weekly_Sales'], color='skyblue')\n",
    "plt.plot(data_Dept.index,data_Dept['Weekly_Sales'], \"o\")\n",
    "plt.title(\"Departmentwise Sales\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xlabel(\"Department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aqui podemos ver que os departamentos 1-15 e 90-95 mostraram vendas semanais mais altas.\n",
    "* Alguns departamentos como 38,40,72 mostraram vendas semanais mais altas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "# Análise de Loja por tipo e por Ano/Mês "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_date_store = df_sales.groupby([\"Date\",\"Store\"]).agg({\"Weekly_Sales\":\"sum\"})\n",
    "sales_date_store.sort_index(inplace=True)\n",
    "sales_date_store.Weekly_Sales = sales_date_store.Weekly_Sales/10000\n",
    "sales_date_store.Weekly_Sales = sales_date_store.Weekly_Sales.apply(int)\n",
    "data_table = pd.merge(df_feature,sales_date_store ,  how='left', on=[\"Date\",\"Store\"])\n",
    "data_table = pd.merge(data_table,df_store[[\"Store\",\"Type\"]] ,  how='left', on=[\"Store\"])\n",
    "data_table.head(20)\n",
    "data_train = data_table[data_table.Weekly_Sales.notnull()]\n",
    "data_test = data_table[data_table.Weekly_Sales.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x=data_train.Date.dt.year, y=data_train.Weekly_Sales,hue=data_train.Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As lojas do tipo A & B apresentaram uma ligeira queda ano a ano na base de vendas.\n",
    "* A loja C apresentou um leve aumento nas vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.barplot(x=data_train.Date.dt.month, y=data_train.Weekly_Sales,hue=data_train.Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As lojas do tipo A e B mostraram pico em novembro e dezembro, principalmente devido ao feriado, enquanto janeiro mostrou queda nas vendas.\n",
    "\n",
    "* As loja do tipo C são consistentes ao longo de cada mês e não apresentaram muitos desvios.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "# Análise MarkDown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "train_markdown = data_table[data_table.MarkDown2.notnull()]\n",
    "train_markdown = train_markdown.groupby(\"Date\").agg({\"MarkDown1\":\"mean\",\"MarkDown2\":\"mean\",\"MarkDown3\":\"mean\",\"MarkDown4\":\"mean\",\"MarkDown5\":\"mean\"})\n",
    "\n",
    "\n",
    "plt.plot(train_markdown.index,train_markdown.MarkDown1)\n",
    "plt.plot(train_markdown.index,train_markdown.MarkDown2)\n",
    "plt.plot(train_markdown.index,train_markdown.MarkDown3)\n",
    "plt.plot(train_markdown.index,train_markdown.MarkDown4)\n",
    "plt.plot(train_markdown.index,train_markdown.MarkDown5)\n",
    "plt.title(\"Timeline Markdown\")\n",
    "plt.ylabel(\"Markdown\")\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_markdown.hist(figsize=(10,8),bins=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_markdown_month = train_markdown.groupby(train_markdown.index.month).agg({\"MarkDown1\":\"mean\",\"MarkDown2\":\"mean\",\"MarkDown3\":\"mean\",\"MarkDown4\":\"mean\",\"MarkDown5\":\"mean\"})\n",
    "\n",
    "train_markdown_month.plot(kind='bar', stacked=True,figsize=(15,6))\n",
    "plt.title(\"Stacked Monthwise Morkdown\")\n",
    "plt.ylabel(\"Markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_markdown_1 = data_table[data_table.MarkDown2.notnull()]\n",
    "train_markdown_type = train_markdown_1.groupby(\"Type\").agg({\"MarkDown1\":\"mean\",\"MarkDown2\":\"mean\",\"MarkDown3\":\"mean\",\"MarkDown4\":\"mean\",\"MarkDown5\":\"mean\"})\n",
    "\n",
    "train_markdown_type.plot(kind='bar', stacked=True,figsize=(10,4))\n",
    "plt.title(\"Stacked StoreType Wise\")\n",
    "plt.ylabel(\"Markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "# **Previsão por Data e Loja**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.linear_model import ElasticNet, Lasso, RidgeCV,LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpeza e pré-processamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def createmodel(data,cols):\n",
    "    for col in cols:\n",
    "        one_hot = pd.get_dummies(data[col],prefix=col)\n",
    "        data = data.join(one_hot)\n",
    "        data.drop(col,axis = 1,inplace=True)\n",
    "    \n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imputando valores faltantes\n",
    "itt = IterativeImputer()\n",
    "df = itt.fit_transform(data_table[[\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\"]]) \n",
    "data_table.MarkDown1 = df[:,0]\n",
    "data_table.MarkDown2 = df[:,1]\n",
    "data_table.MarkDown3 = df[:,2]\n",
    "data_table.MarkDown4 = df[:,3]\n",
    "data_table.MarkDown5 = df[:,4]\n",
    "\n",
    "data_table['CPI'].fillna((data_table['CPI'].mean()), inplace=True)\n",
    "data_table['Unemployment'].fillna((data_table['Unemployment'].mean()), inplace=True)\n",
    "data_table['IsHoliday'] = data_table['IsHoliday'].map({True:0,False:1})\n",
    "\n",
    "# criando nova coluna\n",
    "data_table[\"Month\"] = data_table.Date.dt.month\n",
    "data_table[\"Year\"] = data_table.Date.dt.year\n",
    "data_table[\"WeekofYear\"] = data_table.Date.dt.weekofyear\n",
    "data_table.drop(['Date'],axis=1,inplace=True)\n",
    "\n",
    "# Criando modelos fora da coluna categoria\n",
    "data_table = createmodel(data_table,[\"Type\",\"Month\",\"Year\",\"WeekofYear\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "data_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Divisão de Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_table[data_table.Weekly_Sales.notnull()]\n",
    "data_test = data_table[data_table.Weekly_Sales.isnull()]\n",
    "X = data_train.drop('Weekly_Sales', axis=1)\n",
    "y = data_train['Weekly_Sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criação de modelo básico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LinearRegression(),\n",
    "    ElasticNet(),\n",
    "    RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]),\n",
    "    KernelRidge(alpha=0.6, kernel='polynomial', degree=3, coef0=2.5),\n",
    "    Lasso(alpha =16, random_state=100),\n",
    "    ElasticNet(alpha=0.8),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    SVR(), \n",
    "    LinearSVR(), \n",
    "    NuSVR(),\n",
    "    xgb.XGBRegressor(),\n",
    "    lgb.LGBMRegressor()\n",
    "    ]\n",
    "\n",
    "name = []\n",
    "score = []\n",
    "models = []\n",
    "rmse = []\n",
    "i = 0\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)   \n",
    "    name.append(type(classifier).__name__)\n",
    "    score.append(classifier.score(X_test, y_test))\n",
    "    models.append(classifier)\n",
    "    rmse.append(np.sqrt(mean_squared_error(classifier.predict(X_test), y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparando o desempenho do modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(list(zip(name,rmse, score, models)),columns=['name','rmse','score',\"model\"])\n",
    "df_score.set_index('name',inplace=True)\n",
    "df_score.sort_values(by=['score'],inplace=True)\n",
    "df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previsão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = df_score.loc[\"XGBRegressor\",\"model\"]\n",
    "data_test.drop(['Weekly_Sales'],axis=1,inplace=True)\n",
    "predict = model.predict(data_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
